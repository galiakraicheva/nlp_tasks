{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9dad3b-adcd-44cf-9975-008ee82bb3ed",
   "metadata": {},
   "source": [
    "# Scraping a website with a Python script\n",
    "\n",
    "First, I start with trying the simplest possible option: scraping the website and then using Hugging Face transformers with no LangChain and a small model, optimised for summarisation, BART by facebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f9333c-5082-4d7b-9521-cb2b7ac6a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Artificial intelligence: what it is and how it works\n",
      "Summary: Artificial intelligence is a branch of computer science that develops machine systems capable of demonstrating behaviors linked to human intelligence. The primary benefit of using AI is that these systems can potentially complete tasks better and more efficiently than humans. In order to fully understand what AI is and how it works, one must take into account the current state of AI.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Step 1: Scrape Text from a Website\n",
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract only paragraph (<p>) text to avoid menus, headers and footers\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    text = ' '.join(paragraphs)\n",
    "\n",
    "    return text  # Return only extracted paragraph text\n",
    "\n",
    "# Step 2: Generate Summary and Title using Hugging Face Transformers\n",
    "def summarize_and_title(url):\n",
    "    text = scrape_website(url)\n",
    "    \n",
    "    # Initialize Hugging Face pipelines\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "    title_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", framework=\"pt\")\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summarizer(text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    # Generate title (using the summary as input)\n",
    "    title_prompt = f\"Write a catchy and concise title for the following text:\\n{summary}\"\n",
    "    title = title_generator(title_prompt, max_new_tokens=20, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return summary, title.strip().capitalize()\n",
    "\n",
    "# Example Usage\n",
    "url = \"https://www.wix.com/encyclopedia/definition/artificial-intelligence\"\n",
    "summary, title = summarize_and_title(url)\n",
    "print(f\"Title: {title}\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f62d9-f260-4cb9-a255-36e342ae139c",
   "metadata": {},
   "source": [
    "We see there are problems: The output resembles the first paragraph. Possible reasons for the problem can be that BART has an input limitation of 1024 tokens so the text is truncated and only the small initial part is summarised. In addition, the first paragraph oftentimes have the most important ideas of the article. BART uses wording, close to the original so the output is explainable. Here is how we fix that.\n",
    "\n",
    "#### Truncating the text: \n",
    "\n",
    "The idea is to break the text in smaller parts and these smaller parts to be summarized then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff1241f-4333-4c68-89bc-0710162ea89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 150, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 150, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 150, but your input_length is only 108. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "Your max_length is set to 150, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Your max_length is set to 150, but your input_length is only 108. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "Your max_length is set to 150, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      "Your max_length is set to 150, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      "Your max_length is set to 150, but your input_length is only 110. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      "Your max_length is set to 150, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Artificial intelligence: what it is, what it does, and why\n",
      "Summary: Artificial intelligence is a branch of computer science that develops machine systems capable of demonstrating behaviors linked to human intelligence. The purpose of AI is to improve the systems we already use by automating tasks to make them more efficient. An AI system needs to be built based on three main cognitive skills: Learning, Reasoning and Prediction.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import textwrap  \n",
    "\n",
    "# Step 1: Scrape Text from a Website\n",
    "def scrape_website(url, max_chars=4000):\n",
    "    \"\"\"Extracts paragraph text from a webpage and truncates it to a reasonable length.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract all paragraph texts and join into a single string\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    text = ' '.join(paragraphs)\n",
    "\n",
    "    # Truncate text if necessary\n",
    "    return text[:max_chars] if len(text) > max_chars else text\n",
    "\n",
    "\n",
    "# Step 2: Summarize Text with Chunking\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    \"\"\"Splits long text into smaller chunks to fit model constraints.\"\"\"\n",
    "    return textwrap.wrap(text, width=max_tokens)\n",
    "\n",
    "def summarize_large_text(text):\n",
    "    \"\"\"Summarizes long text in chunks and then summarizes the combined result.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # Generate a summary for each chunk\n",
    "    summaries = [summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "\n",
    "    # If there are multiple summaries, summarize them again\n",
    "    if len(summaries) > 1:\n",
    "        final_summary = summarizer(\" \".join(summaries), max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "    else:\n",
    "        final_summary = summaries[0]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Step 3: Generate Title Based on Summary\n",
    "def generate_title(summary):\n",
    "    \"\"\"Creates a catchy and concise title from the summary.\"\"\"\n",
    "    title_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", framework=\"pt\")\n",
    "    \n",
    "    title_prompt = f\"Write a catchy and concise title for the following text:\\n{summary}\"\n",
    "    title = title_generator(title_prompt, max_new_tokens=20, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return title.strip().capitalize()\n",
    "\n",
    "\n",
    "# Step 4: Main Function\n",
    "def summarize_and_title(url):\n",
    "    \"\"\"Extracts text from a URL, summarizes it, and generates a title.\"\"\"\n",
    "    try:\n",
    "        text = scrape_website(url)\n",
    "        summary = summarize_large_text(text)\n",
    "        title = generate_title(summary)\n",
    "\n",
    "        return title, summary\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", \"\"\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.wix.com/encyclopedia/definition/artificial-intelligence\"\n",
    "    title, summary = summarize_and_title(url)\n",
    "    print(f\"Title: {title}\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87e3dd-b71a-4365-8b8e-59b6b89a28a6",
   "metadata": {},
   "source": [
    "Seems like this is a better summary. The warnings happen because I chunck in characters while the model works with tokens so there is not guarantee that the chuncks will fit in the model's input size. While it is annoying and can lead to unfinished sentences or chunks split at wierd places, it works well on the summary so the result is OK. That's why I will avoid the warnings for now.   \n",
    "\n",
    "#### Making the output longer: \n",
    "\n",
    "However, it is a good idea to make the output a little bit longer since the whole summary is only 3 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a96182f-e7b4-49c2-b95e-c38a8c96de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 100, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 100, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 103, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Your max_length is set to 100, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Artificial intelligence\n",
      "Summary: Artificial intelligence is a branch of computer science that develops machine systems capable of demonstrating behaviors linked to human intelligence. AI programs use data collected from different interactions to improve the way they mimic humans in order to perform tasks such as learning, planning, knowledge representation, perception and problem-solving. Technology is used for a wide range of applications, including inweb development, chatbots for customer service, product recommendations based on user’s habits, speech recognition, and even tobuild a website from scratch.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import textwrap  \n",
    "\n",
    "# Step 1: Scrape Text from a Website\n",
    "def scrape_website(url, max_chars=4000):\n",
    "    \"\"\"Extracts paragraph text from a webpage and truncates it to a reasonable length.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract all paragraph texts and join into a single string\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    text = ' '.join(paragraphs)\n",
    "\n",
    "    # Truncate text if necessary\n",
    "    return text[:max_chars] if len(text) > max_chars else text\n",
    "\n",
    "\n",
    "# Step 2: Summarize Text with Longer Output\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    \"\"\"Splits long text into smaller chunks to fit model constraints.\"\"\"\n",
    "    return textwrap.wrap(text, width=max_tokens)\n",
    "\n",
    "def summarize_large_text(text):\n",
    "    \"\"\"Summarizes long text in chunks and then summarizes the combined result.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_length = len(chunk.split())  # Approximate token count\n",
    "        max_length = max(100, int(input_length * 1.2))  # Increase max_length to ~1.2x input\n",
    "        summary = summarizer(chunk, max_length=max_length, min_length=50, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # If there are multiple summaries, summarize them again\n",
    "    if len(summaries) > 1:\n",
    "        combined_text = \" \".join(summaries)\n",
    "        input_length = len(combined_text.split())\n",
    "        final_max_length = max(100, int(input_length * 1.2))  # Increase final summary length\n",
    "        final_summary = summarizer(combined_text, max_length=final_max_length, min_length=80, do_sample=False)[0]['summary_text']\n",
    "    else:\n",
    "        final_summary = summaries[0]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Step 3: Generate Title Based on Longer Summary\n",
    "def generate_title(summary):\n",
    "    \"\"\"Creates a catchy and concise title from the summary.\"\"\"\n",
    "    title_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", framework=\"pt\")\n",
    "    \n",
    "    title_prompt = f\"Write a catchy and concise title for the following text:\\n{summary}\"\n",
    "    title = title_generator(title_prompt, max_new_tokens=20, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return title.strip().capitalize()\n",
    "\n",
    "\n",
    "# Step 4: Main Function\n",
    "def summarize_and_title(url):\n",
    "    \"\"\"Extracts text from a URL, summarizes it with longer output, and generates a title.\"\"\"\n",
    "    try:\n",
    "        text = scrape_website(url)\n",
    "        summary = summarize_large_text(text)\n",
    "        title = generate_title(summary)\n",
    "\n",
    "        return title, summary\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", \"\"\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.wix.com/encyclopedia/definition/artificial-intelligence\"\n",
    "    title, summary = summarize_and_title(url)\n",
    "    print(f\"Title: {title}\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c68320-1a60-42a6-875c-64d21c34d9fa",
   "metadata": {},
   "source": [
    "#### Dynamically adjusting the max_length: \n",
    "\n",
    "Now I will try to tackle the problem with the warnings. I will try to adjust the max-Length dynamically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1fb16d-e9bb-4997-bbf0-367e20e5749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Artificial intelligence: what it means\n",
      "Summary: Artificial intelligence is a branch of computer science that develops machine systems capable of demonstrating behaviors linked to human intelligence. The primary benefit of using AI is that these systems can potentially complete tasks better and more efficiently than humans. There are four main types of AI, according to a professor at Michigan State University. This categorization spans from the way we’re used to interacting with AI today, to the more “sci-fi” view of how AI might function in the future.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import textwrap  \n",
    "\n",
    "# Step 1: Scrape Text from a Website\n",
    "def scrape_website(url, max_chars=4000):\n",
    "    \"\"\"Extracts paragraph text from a webpage and truncates it to a reasonable length.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract all paragraph texts and join into a single string\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    text = ' '.join(paragraphs)\n",
    "\n",
    "    # Truncate text if necessary\n",
    "    return text[:max_chars] if len(text) > max_chars else text\n",
    "\n",
    "\n",
    "# Step 2: Summarize Text with Safe Length Adjustment\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    \"\"\"Splits long text into smaller chunks to fit model constraints.\"\"\"\n",
    "    return textwrap.wrap(text, width=max_tokens)\n",
    "\n",
    "def safe_max_length(input_length, factor=1.2):\n",
    "    \"\"\"Ensures max_length is safe for summarization\"\"\"\n",
    "    max_length = int(input_length * factor)\n",
    "    \n",
    "    # Ensure max_length does not exceed input length\n",
    "    max_length = min(max_length, input_length - 1)  # Avoid equal-length warnings\n",
    "    max_length = max(30, max_length)  # Ensure at least 30 tokens (for valid summary)\n",
    "    \n",
    "    return max_length\n",
    "\n",
    "def summarize_large_text(text):\n",
    "    \"\"\"Summarizes long text in chunks and then summarizes the combined result.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_length = len(chunk.split())  # Approximate token count\n",
    "        max_length = safe_max_length(input_length, factor=1.2)  # Dynamic & safe length\n",
    "        summary = summarizer(chunk, max_length=max_length, min_length=30, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # If there are multiple summaries, summarize them again\n",
    "    if len(summaries) > 1:\n",
    "        combined_text = \" \".join(summaries)\n",
    "        input_length = len(combined_text.split())\n",
    "        final_max_length = safe_max_length(input_length, factor=1.2)\n",
    "        final_summary = summarizer(combined_text, max_length=final_max_length, min_length=50, do_sample=False)[0]['summary_text']\n",
    "    else:\n",
    "        final_summary = summaries[0]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Step 3: Generate Title Based on Longer Summary\n",
    "def generate_title(summary):\n",
    "    \"\"\"Creates a catchy and concise title from the summary.\"\"\"\n",
    "    title_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", framework=\"pt\")\n",
    "    \n",
    "    title_prompt = f\"Write a catchy and concise title for the following text:\\n{summary}\"\n",
    "    title = title_generator(title_prompt, max_new_tokens=20, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return title.strip().capitalize()\n",
    "\n",
    "\n",
    "# Step 4: Main Function\n",
    "def summarize_and_title(url):\n",
    "    \"\"\"Extracts text from a URL, summarizes it with longer output, and generates a title.\"\"\"\n",
    "    try:\n",
    "        text = scrape_website(url)\n",
    "        summary = summarize_large_text(text)\n",
    "        title = generate_title(summary)\n",
    "\n",
    "        return title, summary\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", \"\"\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.wix.com/encyclopedia/definition/artificial-intelligence\"\n",
    "    title, summary = summarize_and_title(url)\n",
    "    print(f\"Title: {title}\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ce105-6228-434a-a00e-94e32ab8d9e4",
   "metadata": {},
   "source": [
    "This gave rise to only one warning, which is normal because when the text finishes, the length of the chunk will be shorter. \n",
    "\n",
    "#### Trying another chuncking method\n",
    "\n",
    "I will also try a chuncking method that is more compatible with the transformer models and cuts by tokens, not by words or sentences. Since these models work with tokens, it makes more sense to try to use this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db62f9e-47ee-4bd4-aeb3-a5b6021b9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/macbookair/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 150, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 150, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Your max_length is set to 150, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 150, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Your max_length is set to 150, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 150, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 150, but your input_length is only 108. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "Your max_length is set to 150, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 150, but your input_length is only 67. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 150, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 150, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Artificial intelligence: what it means for humans\n",
      "Summary: Artificial intelligence is a branch of computer science that develops machine systems capable of demonstrating behaviors linked to human intelligence. The primary benefit of using AI is that these systems can potentially complete tasks better and more efficiently than humans. Chatbots for customer service, product recommendations based on a user’s habits, speech recognition and even tobuild a website from scratch are all uses of AI.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# Step 1: Scrape Text from a Website\n",
    "def scrape_website(url, max_chars=4000):\n",
    "    \"\"\"Extracts paragraph text from a webpage and truncates it to a reasonable length.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract all paragraph texts and join into a single string\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    text = ' '.join(paragraphs)\n",
    "\n",
    "    # Truncate text if necessary\n",
    "    return text[:max_chars] if len(text) > max_chars else text\n",
    "\n",
    "\n",
    "# Step 2: Split Long Text into Smaller Chunks\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    \"\"\"Splits text into chunks that fit within the model's constraints using sentence boundaries.\"\"\"\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)  # Approximate tokens by character length\n",
    "        \n",
    "        if current_length + sentence_length > max_tokens:\n",
    "            # Store the current chunk\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length\n",
    "\n",
    "    # Add any remaining text as a chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Step 3: Summarize Text in Chunks\n",
    "def summarize_large_text(text):\n",
    "    \"\"\"Summarizes long text in chunks and then summarizes the combined result.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # Generate a summary for each chunk\n",
    "    summaries = [\n",
    "        summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    # If multiple chunks, summarize again\n",
    "    if len(summaries) > 1:\n",
    "        final_summary = summarizer(\" \".join(summaries), max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "    else:\n",
    "        final_summary = summaries[0]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Step 4: Generate a Title from the Summary\n",
    "def generate_title(summary):\n",
    "    \"\"\"Creates a catchy and concise title from the summary.\"\"\"\n",
    "    title_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", framework=\"pt\")\n",
    "    \n",
    "    title_prompt = f\"Write a catchy and concise title for the following text:\\n{summary}\"\n",
    "    title = title_generator(title_prompt, max_new_tokens=20, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return title.strip().capitalize()\n",
    "\n",
    "\n",
    "# Step 5: Main Function\n",
    "def summarize_and_title(url):\n",
    "    \"\"\"Extracts text from a URL, summarizes it, and generates a title.\"\"\"\n",
    "    try:\n",
    "        text = scrape_website(url)\n",
    "        summary = summarize_large_text(text)\n",
    "        title = generate_title(summary)\n",
    "\n",
    "        return title, summary\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", \"\"\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.wix.com/encyclopedia/definition/artificial-intelligence\"\n",
    "    title, summary = summarize_and_title(url)\n",
    "    print(f\"Title: {title}\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab0ad9-4993-4958-adb8-0cad503dcc2c",
   "metadata": {},
   "source": [
    "The summary is OK but the warnings persisted since the text consists of many small sentences like headings and bullet points that can result in chunks that are shorter than the specified lenght. So for this text, the dynamic chunking works the best. However, all the summaries are OK as long as there is chunking, irrespective of the method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
