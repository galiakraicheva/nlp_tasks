{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743d531c-574f-471f-83e3-b810354df98e",
   "metadata": {},
   "source": [
    "# Youtube Video Transcript Summarisation\n",
    "\n",
    "**Task:** Exctract and Summarise the trascript of a Youtube video\n",
    "\n",
    "**Solution:** to summarise a video transcript, I must do the following steps: \n",
    "\n",
    "**Step 1: Get the video transcript**\n",
    "Here I have two options: \n",
    "-Youtube Data API: if it is an official task from an official project, I'd better use Youtube Data API if human-made captions are available for the video. The pros of this is that the offical API is supported by Youtube, meaning that the risk of breaking because Youtube changed something, is minimal. It can also be useful for fetching meta-data. \n",
    "-youtube_transcript_api: a library in Python that can extract the captions even if they are auto-generated. I won't have to authenticate but it is possible that the code can break if Youtube changes something. \n",
    "\n",
    "Since this is not an official project, I don't need meta-data and I don't plan to use it intensively for future needs, I will use the youtube_transcript_api. I prefer to do it quickly and to not deal with authentication. \n",
    "\n",
    "**Step 2: Summarise the transcript with a suitable LLM from Hugging Face**\n",
    "Then I need to pass the video transcript in a suitable LLM for summarisation but I may also have to do some preprocessing of the transcript. \n",
    "\n",
    "#### First model: BART and chuncking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9552a13d-afa9-4853-9e4e-92de00811e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 180, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  creating consistent AI characters is now easier than ever. I've just upgraded my free consistent character workflows based on your feedback making them easier to install and adding so many new cool features. I'm calling this one the consistent Character Creator and here's how it works. This workflow combines several powerful AI tools and models to create this character sheet. At its heart is a tool called multiv view adapter which uses sdxl to generate consistent 3D views of your character. I'll show you how to set everything up step by step at the end of this video. The workflow comes in three versions flux sdxl, sxl light and sXl light which is suitable for lower vram environments. It allows you to create a wide variety of styles and it's also a very good balance between speed and quality. Once you have everything set up you can just drag and drop the workflow file into the com UI interface. The workflow is designed to make it easier for players to create their own characters. It will automatically generate different views for your character based on this prompt. You can also create a new character using this workflow using the node here to one otherwise it will just take a look at this image. Make sure that you are using these correct pipelines so you want to use the under adapter name you it should be this one and under pipeline name it should been this one the next thing that this part will do it will extract the face of your character and put it in front of a gray background. The values usually work pretty well and you can see that up here you can compare the input image and the generated image that worked really well. that higher the original image will influence your image more and keep it more consistent but it will also not allow the AI to add additional detail and create that quality. This is only the first step because next to that here we have the second up scaling group this first part here will upscale the image to a value that you specify here so I'm doubling the size and using the Doo's value again you can decide how much the AI is allowed to change the image so this one usually works really well as a starting point. In this group the emotions for your character will be created and you can super easily change them by just playing around with the sliders so for example let's make her Wing here and maybe raise the eyebrows. Next to that we have the lighting group and the way this one works is you can go up here and create some different environments that you want to put your character in. If you don't want to use an image as your input you simply need to go to the top here switch on character generation and change this one here to one so it will select this first input download the free o. u can see this just works really really well and the character is absolutely consistent if you want. A consistent character creator can allow you to change the style of an input image. For example you could change the art style from photography to pixel 3D animation. You can also download a Laura for a specific style for example on civit AI and load it in here. You can basically input any style that you like for example I could load in this character right here sort of already like a Disney style click Q prompt and it will create this character sheet for me. You don't even need to input an image of a character you can also just input like a product or something and the workflow will generate a character sheet. The free version of the flux version of this workflow is structured in the exact same way as the flex version. You need to make sure that you have a flux checkpoint loaded and you can can download that via the com UI model manager if you want. The second thing you need to change is you need the instant X control net for flux. If you want the maximum quality out of these workflows consider supporting me on patreon. Flux is a lot better with like human anatomy and it will fix the body parts way better than for example the sdxl version. We also don't have a functioning IP adapter so I remove that part and that means you cannot go as high with the den noising however in my experience it works really well at like 25%. The low vram versions of all the workflows have the same structure but with a key difference the multi view adapter part is missing. This one requires at least 12 GB of vram but I still want you to be able to use the workflow. The way it works is you import your character image right here. Aura is a way to give an AI model more context for a specific object style or character to show you how this works I created this pixel Style character using this input image right here and these are the character sheets that it created so now I create my data set by just selecting the best images. I definitely want images from from the side and from the back this one looks great as well. Lara will be given a name and a trigger word to be used in prompts. Lara will also be captioned so that flux knows what's depicted in these images. This will just give us more flexibility when we work with this character. We can use Florence 2 in my consistent character workflow to create the captions. Florence 2 just did a pretty good job of captioning these images but I would like to be even more precise. I now also want to name her emotions and the view angle. I'm going to add T pose and standing with arms outstretched okay so these are my final captions and now I can just click start training. You need to refresh or restart comi and then I just load in my standard flx image Generation Um workflow we're loading in our flx Dev checkpoint here and then we need to load in our Laura here. Make sure to drop in the keyword that you created here and you can already see it's following our prompt perfectly here. I also added the option to add additional detail using these uh detail demon uh sampler settings here. Free guide shows you the full installation process for comi. Let's start by downloading comi and installing git. Then we need to download the com VII manager so just go to this page scroll down to installation and rightclick on this link save link as and go to your newly created com UI directory. With the newest version of compi some of the custom nodes weren't installing properly this will probably be fixed in the future but for now you have to use this workaround here you can find it in my free guide. All you need to do is go to your comi directory go to the address bar and type in CMD to open this command window. You can find all the models that you need to download right here to the left left of the workflow so you can see directly next to the the corresponding note where to get that model. You can also go to the bottom corner here and um toggle link visibility so you don't see these Links at all and it will look even cleaner that way. We need an SD 1.5 checkpoint and we need that for the relighting process for IC light and it's not really important which one we use so I recommend just downloading Photon. We also need to download the models for the IP adapter and you can also do that in the manager just go to model maner search for IP adapter. Next we need the clip models for the IP adapter so just go to clip scroll down until you see this here clip Vision model needed for IP adapter and so you need to download these ones right here and you also need to install these models here. Once that's done you can click up to refresh comi or restart now for the newest version of comi. Thank you so much and see you next time. I'll be in touch via e-mail or text. I hope to see you in the next few days. I'm looking forward to seeing you all again. I love you all very much.\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text = \" \".join([entry['text'] for entry in transcript])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def summarize_text(text, max_words=180):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    chunk_size = 1024  \n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    \n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=max_words, min_length=50, do_sample=False)\n",
    "        summary.append(summarized_chunk[0]['summary_text'])\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "# Example Usage\n",
    "video_id = \"grtmiWbmvv0\" #Video ID goes here\n",
    "transcript = get_youtube_transcript(video_id)\n",
    "\n",
    "if \"Error\" not in transcript:\n",
    "    summary = summarize_text(transcript)\n",
    "    print(\"\\nSummary:\\n\", summary)\n",
    "else:\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1860d-9bb9-4325-b7f9-66df4f8861b6",
   "metadata": {},
   "source": [
    "It works well. Here and there there are some mistakes or repetitions but it is normal since the model is not very big. Indeed it has summarised the key points in the transcript, keeping some of the original wording, which is normal for the BART model. \n",
    "\n",
    "It gave the same warning messages as in the [Scraping website task](https://github.com/galiakraicheva/nlp_tasks/blob/main/websummarisation/bart_websummarisation.ipynb). However, I this time, I just shortened the max_lenght and it didn't change the summary a lot. \n",
    "\n",
    "After trying with more examples, the code seems to summarise relatively long transcripts well.  \n",
    "\n",
    "**A side note:** \n",
    "\n",
    "A thing to improve in this script is the way you reference the video. Now I am using video ID. But where do we find our video ID? It is in the url of the video on Youtube. \n",
    "\n",
    "https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
    "\n",
    "It is 11 characters long and is after ?v=. \n",
    "\n",
    "However, it would be very helpful to do it automatically: to paste the video url and let Python figure out by itself what the video ID is. \n",
    "\n",
    "#### Testing the model on short lyrics:\n",
    "\n",
    "The model summarises well tutorials and longer transcripts. However, we should test also for very short and repetitive transcripts like song lyrics, audio-poems, etc. First, I test with: \n",
    "\n",
    "#### Second model: BART, chuncking and automated video ID retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4468eb60-301b-4e98-af0b-38bf39546cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " \"We're no strangers to love you know the rules and so do I I full commitments while I'm thinking of you wouldn't get this from any other guy\" \"We've known each other for so long your heart's been aching but your to sh to say it inside\" \"If you ask me how I'm feeling don't tell me you're too my\" \"I just want to tell you how I'm feeling got to make you understand\" \"Your heart's been aching but you're too sh to say inside\" \"We both know what's been going on we the game and we're going to play it\"\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text = \" \".join([entry['text'] for entry in transcript])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def summarize_text(text, max_words=180):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    chunk_size = 1024  \n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    \n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=max_words, min_length=50, do_sample=False)\n",
    "        summary.append(summarized_chunk[0]['summary_text'])\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r\"(?:v=|\\/embed\\/|\\/shorts\\/|\\/watch\\?v=|youtu\\.be\\/|\\/v\\/|\\/e\\/|\\/\\?v=|\\/\\?feature=player_embedded&v=|&v=|\\/\\#\\/watch\\?v=)([a-zA-Z0-9_-]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Example Usage\n",
    "youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "video_id = extract_video_id(youtube_url)\n",
    "transcript = get_youtube_transcript(video_id)\n",
    "\n",
    "if \"Error\" not in transcript:\n",
    "    summary = summarize_text(transcript)\n",
    "    print(\"\\nSummary:\\n\", summary)\n",
    "else:\n",
    "    print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009c20b-44ce-4332-82ba-af28cd6119c0",
   "metadata": {},
   "source": [
    "What is obvious from the output is that it is very different from the one before. Before, it was long and there were no broken sentences. Now, it is very short and there are broken sentences. The lenght difference is normal since the first video is a tutorial, whereas the second video is a song and the lyrics are much shorter. Looking at the song lyrics I see lots of repetitions. So it is good to make the code robust to repetitions and cut sentences. To do that: \n",
    "\n",
    "1) Removing repetitive lines because the model may find it hard to identify what to remove and generates unnatural breaks. If I remove the repetitive lyrics, I will help the model focus on meaning.\n",
    "2) Split by full sentences instead of raw characters so there won't be any words split in half and incomplete chunks.\n",
    "\n",
    "#### Third model: BART, full sentences and removed repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841b12c5-7d9c-4639-aaf3-fae391c3a861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 180, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots. Please share your best photos of the United States with CNN iReport. \"We've known each other for so long your heart's been aching but you're too sh to say inside\" \"We're no strangers to love you know the rules and so do I I full commitments while I'm thinking of you wouldn't get this from any other guy\" \"I just want to tell you how I'm feeling got to make you understand Never Going To Give You Up\"\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text = \" \".join([entry['text'] for entry in transcript])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def remove_redundant_lines(text):\n",
    "    \"\"\" Remove duplicate lines in song lyrics to improve summarization \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    unique_lines = list(dict.fromkeys(lines))\n",
    "    return \" \".join(unique_lines)\n",
    "\n",
    "def chunk_sentences(text, max_length=1024):\n",
    "    \"\"\" Split text into chunks by sentences, not raw character count \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence  # Start new chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text, max_words=180):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    text = remove_redundant_lines(text)  # Remove repeated lines\n",
    "    chunks = chunk_sentences(text)  # Split by sentence\n",
    "\n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=max_words, min_length=50, do_sample=False)\n",
    "        summary.append(summarized_chunk[0]['summary_text'])\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r\"(?:v=|\\/embed\\/|\\/shorts\\/|\\/watch\\?v=|youtu\\.be\\/|\\/v\\/|\\/e\\/|\\/\\?v=|\\/\\?feature=player_embedded&v=|&v=|\\/\\#\\/watch\\?v=)([a-zA-Z0-9_-]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Example Usage\n",
    "youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "video_id = extract_video_id(youtube_url)\n",
    "transcript = get_youtube_transcript(video_id)\n",
    "\n",
    "if \"Error\" not in transcript:\n",
    "    summary = summarize_text(transcript)\n",
    "    print(\"\\nSummary:\\n\", summary)\n",
    "else:\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb00e1f-f824-4812-952c-ef2be4a1c23b",
   "metadata": {},
   "source": [
    "Instead of improving, the model failed miserably by including some CNN data. This is because when removing the repetitive lyrics, the input text of the model became really little and BART, trained originally on CNN data hallucinates some of its training data. It is a good idea to try another model that handles short data better (or alternatively leave the removed duplicates and just fix the text spliting)\n",
    "\n",
    "#### Fourth model: trying t-5-base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1baea1-b878-4e46-82f6-7b79ee6fe5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 80, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " . - (EN) - we've known each other for so long your heart's been aching but your to sh to say it inside we both know what's going on we know the game and we're going to play it . if you ask me how I'm feeling don't tell me you're too my you see Never Going To Give You Up never going to let you\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text = \" \".join([entry['text'] for entry in transcript])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def remove_redundant_lines(text):\n",
    "    \"\"\" Remove duplicate lines in song lyrics to improve summarization \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    unique_lines = list(dict.fromkeys(lines))\n",
    "    return \" \".join(unique_lines)\n",
    "\n",
    "def chunk_sentences(text, max_length=1024):\n",
    "    \"\"\" Split text into chunks by sentences, not raw character count \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence  # Start new chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text, max_words=80):\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "    \n",
    "    text = remove_redundant_lines(text)  # Remove repeated lines\n",
    "    chunks = chunk_sentences(text)  # Split by sentence\n",
    "\n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=max_words, min_length=10, do_sample=False)\n",
    "        summary.append(summarized_chunk[0]['summary_text'])\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r\"(?:v=|\\/embed\\/|\\/shorts\\/|\\/watch\\?v=|youtu\\.be\\/|\\/v\\/|\\/e\\/|\\/\\?v=|\\/\\?feature=player_embedded&v=|&v=|\\/\\#\\/watch\\?v=)([a-zA-Z0-9_-]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Example Usage\n",
    "youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "video_id = extract_video_id(youtube_url)\n",
    "transcript = get_youtube_transcript(video_id)\n",
    "\n",
    "if \"Error\" not in transcript:\n",
    "    summary = summarize_text(transcript)\n",
    "    print(\"\\nSummary:\\n\", summary)\n",
    "else:\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136bd94-0a58-4240-a704-01d48c01bf44",
   "metadata": {},
   "source": [
    "Here I have used a different model and I have decreased the length of the summary but it is good to test also to summarise the summary to prevent cutting the summary. \n",
    "\n",
    "#### Fifth model: recursive summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16406466-0280-4d9c-bdfa-211853463f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 120, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " . - (EN) - we've known each other for so long your heart's been aching but your to sh to say it inside we both know what's going on we know the game and we're going to play it . if you ask me how I'm feeling don't tell me you're too my you see Never Going To Give You Up never going to let you\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text = \" \".join([entry['text'] for entry in transcript])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def remove_redundant_lines(text):\n",
    "    \"\"\" Remove duplicate lines in song lyrics to improve summarization \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    unique_lines = list(dict.fromkeys(lines))\n",
    "    return \" \".join(unique_lines)\n",
    "\n",
    "def chunk_sentences(text, max_length=1024):\n",
    "    \"\"\" Split text into chunks by sentences, not raw character count \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence  # Start new chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text, max_words=120):\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "    \n",
    "    text = remove_redundant_lines(text)  # Remove repeated lines\n",
    "    chunks = chunk_sentences(text)  # Split by sentence\n",
    "\n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=max_words, min_length=10, do_sample=False)\n",
    "        summary.append(summarized_chunk[0]['summary_text'])\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "# Recursive Shortening\n",
    "def recursive_summarization(text, max_words=100):\n",
    "    first_summary = summarize_text(text, max_words=max_words)  \n",
    "    shorter_summary = summarize_text(first_summary, max_words=max_words//2)  \n",
    "    return shorter_summary\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r\"(?:v=|\\/embed\\/|\\/shorts\\/|\\/watch\\?v=|youtu\\.be\\/|\\/v\\/|\\/e\\/|\\/\\?v=|\\/\\?feature=player_embedded&v=|&v=|\\/\\#\\/watch\\?v=)([a-zA-Z0-9_-]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Example Usage\n",
    "youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "video_id = extract_video_id(youtube_url)\n",
    "transcript = get_youtube_transcript(video_id)\n",
    "\n",
    "if \"Error\" not in transcript:\n",
    "    recursive_summarization(transcript, max_words=120)\n",
    "    print(\"\\nSummary:\\n\", summary)\n",
    "else:\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db2e2f-56cc-4129-a46c-3e32315484e9",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "\n",
    "**For longer text:** the first model seemed to work well. \n",
    "\n",
    "**For shorter text:** the second model performed the best so far. Removing repetitions doesn't seem urgent because they don't show up in the summary and if I do, the text becomes really short which messes up the summarisation. As further steps, it can be helpful to try stronger models on short text, like GPT-4. \n",
    "\n",
    "Since transcripts can be very different in topics, length, wording and vocabulary, it is a good idea, instead of looking for a universal model to summarise all transcripts, to fine-tune a model on specific types of transcripts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
